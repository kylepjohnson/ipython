{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following tutorial [\"Topic Modeling for Fun and Profit\"](http://radimrehurek.com/topic_modeling_tutorial/2%20-%20Topic%20Modeling.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import logging\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "from cltk.stop.greek.stops import STOPS_LIST\n",
    "import gensim\n",
    "from gensim.corpora.mmcorpus import MmCorpus\n",
    "from gensim.utils import simple_preprocess\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(format='%(levelname)s : %(message)s', level=logging.INFO)\n",
    "logging.root.level = logging.INFO  # ipython sometimes messes up the logging setup; restore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user_dir = os.path.expanduser('~/cltk_data/user_data/lda_tlg/')\n",
    "try:\n",
    "    os.makedirs(user_dir)\n",
    "except FileExistsError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PREPROCESS_DEACCENT = False\n",
    "STOPS_LIST = [simple_preprocess(stop, deacc=PREPROCESS_DEACCENT)[0] for stop in STOPS_LIST if len(simple_preprocess(stop, deacc=PREPROCESS_DEACCENT)) > 0]\n",
    "STOPS_LIST = ['τῆϲ', 'τοῖϲ', 'εἰϲ', 'πρὸϲ', 'τοὺϲ']\n",
    "STOPS_LIST += [\"τηϲ\", \"τοιϲ\", \"εϲτι\", \"προϲ\", \"ειϲ\", \"ταϲ\", \"ωϲ\", \"τουϲ\", \"ξυν\", 'πρε']  # useful for after rm accents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TOK_MIN = 3  # rm words shorter than\n",
    "TOK_MAX = 20  # rm words longer than\n",
    "DOC_MIN = 50  # drop docs shorter than\n",
    "def tokenize(text):\n",
    "    \"\"\"Tokenize and rm stopwords. The Gensim `simple_preprocess` will work fine\n",
    "    here becuase the Greek text has already been aggressively cleaned up.\n",
    "    https://radimrehurek.com/gensim/utils.html#gensim.utils.simple_preprocess\n",
    "    \"\"\"\n",
    "    tokens = [token for token in simple_preprocess(text, deacc=PREPROCESS_DEACCENT, min_len=TOK_MIN, max_len=TOK_MAX)]\n",
    "    return [token for token in tokens if token not in STOPS_LIST]\n",
    "    \n",
    "\n",
    "def iter_tlg(tlg_dir):\n",
    "    \"\"\"Stream TLG doc-by-doc.\"\"\"\n",
    "    file_names = os.listdir(tlg_dir)\n",
    "    for file_name in file_names:\n",
    "        file_path = os.path.join(tlg_dir, file_name)\n",
    "        with open(file_path) as file_open:\n",
    "            file_read = file_open.read()\n",
    "        tokens = tokenize(file_read)\n",
    "        # ignore very short docs\n",
    "        # todo: get file length distribution to better know what is short in TLG\n",
    "        if len(tokens) < DOC_MIN:\n",
    "            continue\n",
    "        yield file_name, tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Take a look at the docs post-processing\n",
    "# Open corpus iterator\n",
    "tlg_preprocessed = os.path.expanduser('~/cltk_data/greek/text/tlg/plaintext/')\n",
    "stream = iter_tlg(tlg_preprocessed)\n",
    "for title, tokens in itertools.islice(iter_tlg(tlg_preprocessed), 8):\n",
    "    print(title, tokens[:10])  # print the article title and its first ten tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mk word dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Open corpus iterator\n",
    "doc_stream = (tokens for _, tokens in iter_tlg(tlg_preprocessed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "no_below = 20\n",
    "no_above = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# store the dictionary, for future reference\n",
    "dict_name = 'gensim_dict_id2word_tlg_nobelow{}_noabove{}_tokmin{}_tokmax{}_docmin{}_deaccent{}.dict'.format(no_below, \n",
    "                                                                                                            no_above, \n",
    "                                                                                                            TOK_MIN, \n",
    "                                                                                                            TOK_MAX, \n",
    "                                                                                                            DOC_MIN, \n",
    "                                                                                                            PREPROCESS_DEACCENT)\n",
    "dict_path = os.path.join(user_dir, dict_name)\n",
    "\n",
    "try:\n",
    "    id2word_tlg = gensim.corpora.dictionary.Dictionary.load(dict_path)\n",
    "except FileNotFoundError:\n",
    "    t0 = time.time()\n",
    "    # ~4 min on TLG corpus if rm accents; ~w min if not\n",
    "    id2word_tlg = gensim.corpora.Dictionary(doc_stream)\n",
    "    # this cutoff might lose too much info, we'll see\n",
    "    # ignore words that appear in less than 20 documents or more than 10% documents\n",
    "    id2word_tlg.filter_extremes(no_below=no_below, no_above=no_above)\n",
    "    id2word_tlg.save(dict_path)\n",
    "    print('Time to mk new corpus dictionary:', time.time() - t0)\n",
    "print(id2word_tlg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mk vectors\n",
    "\n",
    "Now start again with the corpus, turning the actual words into integers from our map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Illustrate what this BoW space looks like with example doc\n",
    "doc = \"περὶ ποιητικῆς αὐτῆς τε καὶ τῶν εἰδῶν αὐτῆς, ἥν τινα δύναμιν ἕκαστον ἔχει, καὶ πῶς δεῖ συνίστασθαι τοὺς μύθους [10] εἰ μέλλει καλῶς ἕξειν ἡ ποίησις, ἔτι δὲ ἐκ πόσων καὶ ποίων ἐστὶ μορίων, ὁμοίως δὲ καὶ περὶ τῶν ἄλλων ὅσα τῆς αὐτῆς ἐστι μεθόδου, λέγωμεν ἀρξάμενοι κατὰ φύσιν πρῶτον ἀπὸ τῶν πρώτων.\"\n",
    "doc = ' '.join(simple_preprocess(doc))\n",
    "bow = id2word_tlg.doc2bow(tokenize(doc))\n",
    "print(bow)  # words both in BoW dict and doc\n",
    "print(id2word_tlg[bow[0][0]])  # map int back to str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TLGCorpus(object):\n",
    "    def __init__(self, dump_file, dictionary, clip_docs=None):\n",
    "        \"\"\"Yield each document in turn, as a list of tokens (unicode strings).\n",
    "        \"\"\"\n",
    "        self.dump_file = dump_file\n",
    "        self.dictionary = dictionary\n",
    "        self.clip_docs = clip_docs\n",
    "    \n",
    "    def __iter__(self):\n",
    "        self.titles = []\n",
    "        for title, tokens in itertools.islice(iter_tlg(self.dump_file), self.clip_docs):\n",
    "            self.titles.append(title)\n",
    "            yield self.dictionary.doc2bow(tokens)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.clip_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make the BoW corpus\n",
    "# creates a stream of bag-of-words vectors\n",
    "corpus_bow_tlg = TLGCorpus(tlg_preprocessed, id2word_tlg)\n",
    "\n",
    "# reduce corpus size for faster testing\n",
    "#corpus_bow_tlg = gensim.utils.ClippedCorpus(corpus_bow_tlg, 100)\n",
    "\n",
    "# vector = next(iter(corpus_bow_tlg))\n",
    "# print(vector)  # print the first vector in the stream\n",
    "# [(0, 1), (1, 1), (2, 1), ...]\n",
    "\n",
    "# # what is the most common word in that first article?\n",
    "# most_index, most_count = max(vector, key=lambda _tuple: _tuple[1])\n",
    "# print(id2word_tlg[most_index], most_count)  # μιλησιοις 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save BoW\n",
    "# ~4 min on TLG corpus\n",
    "bow_name = 'gensim_bow_tlg_nobelow{}_noabove{}_tokmin{}_tokmax{}_docmin{}_deaccent{}.mm'.format(no_below, \n",
    "                                                                                                no_above, \n",
    "                                                                                                TOK_MIN, \n",
    "                                                                                                TOK_MAX, \n",
    "                                                                                                DOC_MIN, \n",
    "                                                                                                PREPROCESS_DEACCENT)\n",
    "bow_path = os.path.join(user_dir, bow_name)\n",
    "t0 = time.time()\n",
    "gensim.corpora.MmCorpus.serialize(bow_path, corpus_bow_tlg)\n",
    "print('Time to save BoW space:', time.time() - t0)\n",
    "\n",
    "# Later load saved corpus with:\n",
    "# corpus_bow_tlg = gensim.corpora.MmCorpus(bow_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Quick testing using just a part of the corpus\n",
    "\n",
    "NUM_TOPICS_LIST = [5, 10, 20, 40, 60, 120]\n",
    "PASSES = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for num_topics in NUM_TOPICS_LIST:\n",
    "    print('Beginning training ...')\n",
    "    print('... {} topics and {} passes ...'.format(num_topics, PASSES))\n",
    "    t0 = time.time()\n",
    "    lda_model = gensim.models.LdaMulticore(corpus_bow_tlg, num_topics=num_topics, id2word=id2word_tlg, passes=PASSES)\n",
    "    \n",
    "    # save LDA vector space\n",
    "    lda_space_name = 'gensim_lda_space_tlg_numtopics{}_numpasses{}_nobelow{}_noabove{}_tokmin{}_tokmax{}_docmin{}_deaccent{}.mm'.format(num_topics, \n",
    "                                                                                                                                        PASSES, \n",
    "                                                                                                                                        no_below, \n",
    "                                                                                                                                        no_above, \n",
    "                                                                                                                                        TOK_MIN, \n",
    "                                                                                                                                        TOK_MAX, \n",
    "                                                                                                                                        DOC_MIN, \n",
    "                                                                                                                                        PREPROCESS_DEACCENT)\n",
    "    path_lda = os.path.join(user_dir, lda_space_name)\n",
    "    gensim.corpora.MmCorpus.serialize(path_lda, lda_model[corpus_bow_tlg])\n",
    "    \n",
    "    # save model\n",
    "    lda_model_name = 'gensim_lda_model_tlg_numtopics{}_numpasses{}_nobelow{}_noabove{}_tokmin{}_tokmax{}_docmin{}_deaccent{}.model'.format(num_topics, \n",
    "                                                                                                                                           PASSES, \n",
    "                                                                                                                                           no_below, \n",
    "                                                                                                                                           no_above, \n",
    "                                                                                                                                           TOK_MIN, \n",
    "                                                                                                                                           TOK_MAX, \n",
    "                                                                                                                                           DOC_MIN, \n",
    "                                                                                                                                           PREPROCESS_DEACCENT)\n",
    "    path_lda = os.path.join(user_dir, lda_model_name)\n",
    "    lda_model.save(path_lda)\n",
    "    print('Time to train LDA model space:', time.time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Examples of how to use the model\n",
    "# lda_model.print_topics(-1)  # print a few most important words for each LDA topic\n",
    "# # transform text into the bag-of-words space\n",
    "# bow_vector = id2word_tlg.doc2bow(tokenize(doc))\n",
    "# print([(id2word_tlg[id], count) for id, count in bow_vector])\n",
    "\n",
    "# # transform into LDA space\n",
    "# lda_vector = lda_model[bow_vector]\n",
    "# print(lda_vector)\n",
    "\n",
    "# # print the document's single most prominent LDA topic\n",
    "# print(lda_model.print_topic(max(lda_vector, key=lambda item: item[1])[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word intrusion\n",
    "\n",
    "> For each trained topic, they take its first ten words, then substitute one of them with another, randomly chosen word (intruder!) and see whether a human can reliably tell which one it was. If so, the trained topic is topically coherent (good); if not, the topic has no discernible theme (bad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for num_topics in NUM_TOPICS_LIST:\n",
    "    # load model\n",
    "    lda_model_name = 'gensim_lda_model_tlg_numtopics{}_numpasses{}_nobelow{}_noabove{}_tokmin{}_tokmax{}_docmin{}_deaccent{}.model'.format(num_topics, \n",
    "                                                                                                                                           PASSES, \n",
    "                                                                                                                                           no_below, \n",
    "                                                                                                                                           no_above, \n",
    "                                                                                                                                           TOK_MIN, \n",
    "                                                                                                                                           TOK_MAX, \n",
    "                                                                                                                                           DOC_MIN, \n",
    "                                                                                                                                           PREPROCESS_DEACCENT)\n",
    "    print('Loading model: {} ...'.format(lda_model_name))\n",
    "    print('... for word intrusion testing ...')\n",
    "    path_lda = os.path.join(user_dir, lda_model_name)\n",
    "    lda_model = gensim.models.LdaMulticore.load(path_lda)\n",
    "    \n",
    "    # select top 50 words for each of the LDA topics\n",
    "    print('Top 50 words of each LDA model:')\n",
    "    top_words = [[word for word, _ in lda_model.show_topic(topicno, topn=50)] for topicno in range(lda_model.num_topics)]\n",
    "    print(top_words)\n",
    "    print('')\n",
    "\n",
    "    # get all top 50 words in all 20 topics, as one large set\n",
    "    all_words = set(itertools.chain.from_iterable(top_words))\n",
    "    print(\"Can you spot the misplaced word in each topic?\")\n",
    "\n",
    "    # for each topic, replace a word at a different index, to make it more interesting\n",
    "    replace_index = np.random.randint(0, 10, lda_model.num_topics)\n",
    "\n",
    "    replacements = []\n",
    "    for topicno, words in enumerate(top_words):\n",
    "        other_words = all_words.difference(words)\n",
    "        replacement = np.random.choice(list(other_words))\n",
    "        replacements.append((words[replace_index[topicno]], replacement))\n",
    "        words[replace_index[topicno]] = replacement\n",
    "        print(\"%i: %s\" % (topicno, ' '.join(words[:10])))\n",
    "    \n",
    "    print(\"Actual replacements were:\")\n",
    "    print(list(enumerate(replacements)))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split doc\n",
    "\n",
    "> We'll split each document into two parts, and check that 1) topics of the first half are similar to topics of the second 2) halves of different documents are mostly dissimilar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# evaluate on 1k documents **not** used in LDA training\n",
    "tlg_preprocessed = os.path.expanduser('~/cltk_data/greek/text/tlg/plaintext/')\n",
    "doc_stream = (tokens for _, tokens in iter_tlg(tlg_preprocessed))  # generator\n",
    "test_docs = list(itertools.islice(doc_stream, 100, 200))  # ['πανυ', 'καλως', ...], [...], ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def intra_inter(model, test_docs, num_pairs=10000):\n",
    "    # split each test document into two halves and compute topics for each half\n",
    "    part1 = [model[id2word_tlg.doc2bow(tokens[: len(tokens) // 2])] for tokens in test_docs]\n",
    "    part2 = [model[id2word_tlg.doc2bow(tokens[len(tokens) // 2 :])] for tokens in test_docs]\n",
    "    \n",
    "    # print computed similarities (uses cossim)\n",
    "    print(\"average cosine similarity between corresponding parts (higher is better):\")\n",
    "    print(np.mean([gensim.matutils.cossim(p1, p2) for p1, p2 in zip(part1, part2)]))\n",
    "\n",
    "    random_pairs = np.random.randint(0, len(test_docs), size=(num_pairs, 2))\n",
    "    print(\"average cosine similarity between {} random parts (lower is better):\".format(num_pairs))    \n",
    "    print(np.mean([gensim.matutils.cossim(part1[i[0]], part2[i[1]]) for i in random_pairs]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for num_topics in NUM_TOPICS_LIST:\n",
    "    # load model\n",
    "    lda_model_name = 'gensim_lda_model_tlg_numtopics{}_numpasses{}_nobelow{}_noabove{}_tokmin{}_tokmax{}_docmin{}_deaccent{}.model'.format(num_topics, \n",
    "                                                                                                                                           PASSES, \n",
    "                                                                                                                                           no_below, \n",
    "                                                                                                                                           no_above, \n",
    "                                                                                                                                           TOK_MIN, \n",
    "                                                                                                                                           TOK_MAX, \n",
    "                                                                                                                                           DOC_MIN, \n",
    "                                                                                                                                           PREPROCESS_DEACCENT)\n",
    "    print('Loading model: {} ...'.format(lda_model_name))\n",
    "    print('... for testing split document topic matching ...')\n",
    "    path_lda = os.path.join(user_dir, lda_model_name)\n",
    "    lda_model = gensim.models.LdaMulticore.load(path_lda)\n",
    "\n",
    "    print(\"LDA results:\")\n",
    "    intra_inter(lda_model, test_docs)\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform all docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for num_topics in NUM_TOPICS_LIST:\n",
    "    print('num topics', num_topics)\n",
    "    # load model\n",
    "    lda_model_name = 'gensim_lda_model_tlg_numtopics{}_numpasses{}_nobelow{}_noabove{}_tokmin{}_tokmax{}_docmin{}_deaccent{}.model'.format(num_topics, \n",
    "                                                                                                                                           PASSES, \n",
    "                                                                                                                                           no_below, \n",
    "                                                                                                                                           no_above, \n",
    "                                                                                                                                           TOK_MIN, \n",
    "                                                                                                                                           TOK_MAX, \n",
    "                                                                                                                                           DOC_MIN, \n",
    "                                                                                                                                           PREPROCESS_DEACCENT)\n",
    "    print('Loading model: {} ...'.format(lda_model_name))\n",
    "    print('... scoring topics of all TLG documents ...')\n",
    "    path_lda = os.path.join(user_dir, lda_model_name)\n",
    "    lda_model = gensim.models.LdaMulticore.load(path_lda)\n",
    "\n",
    "    # mk save path name\n",
    "    scores_name = lda_model_name.rstrip('.model') + '.scores'\n",
    "    scores_path = os.path.join(user_dir, scores_name)\n",
    "    doc_topics = ''\n",
    "    for title, tokens in iter_tlg(tlg_preprocessed):\n",
    "        #print(title, tokens[:10])  # print the article title and its first ten tokens\n",
    "        # print(title)\n",
    "        topic_distribution = str(lda_model[id2word_tlg.doc2bow(tokens)])\n",
    "        # print(topic_distribution)\n",
    "        doc_topics += 'title: ' + title + '\\n'\n",
    "        doc_topics += topic_distribution + '\\n\\n'\n",
    "    with open(scores_path, 'w') as file_open:\n",
    "        file_open.write(doc_topics)\n",
    "    print('')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
