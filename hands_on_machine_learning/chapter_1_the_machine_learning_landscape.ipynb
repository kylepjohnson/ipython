{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Types of ML systems\n",
    "\n",
    "* trained w/ or w/o human supervision (supervised, unsupervised, semi-supervised, reinforcement learning)\n",
    "* can or cannot learn learn incrementally on the fly (online v batch learning)\n",
    "* comparing new data points to known, or detect patterns in training data and build predictive model (instance-based v model-based learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised/Unsupervised learning\n",
    "\n",
    "## Supervised\n",
    "\n",
    "* *labels*: solutions fed to algo along with training data\n",
    "* *classification*: algo is trained w/ many examples with their *class*\n",
    "* *regression*: predict a *target* numeric value given a set of *features* called *predictors*\n",
    "* *attribute*: a data type (eg, \"mileage\")\n",
    "* *feature*: an attribute plus its value (eg, \"mileage = 15,000\")\n",
    "* logistic regression can be used for classification, as it outputs a value that corresponds to the probability of belonging to a given class\n",
    "\n",
    "* most important supervised learning algos\n",
    "  - k-nearest neighbors\n",
    "  - linear regression\n",
    "  - logistic regression\n",
    "  - support vector machines (SVMs)\n",
    "  - decision trees and random forests\n",
    "  - neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised learning\n",
    "\n",
    "* clustering\n",
    "  - k-means\n",
    "  - hierarchical cluster analysis (HCA)\n",
    "  - expectation maximization\n",
    "* visualization and dimensionality reduction\n",
    "  - principal component analysis (PCA)\n",
    "  - kernel PCE\n",
    "  - locally-linear embedding (LLE)\n",
    "  - t-distributed stochastic neighbor embedding (t-SNE)\n",
    "* association rule learning\n",
    "  - apriori\n",
    "  - eclat\n",
    "* *dimensionalty reduction*: simplify data w/o loosing to much info\n",
    "  - example: a car's mileage highly correlated w/ its age, so merge them into one feature\n",
    "* *feature extraction*: building informative and non-redundant features from an initial set of measured data\n",
    "* *anomaly detection*: identifying outliers\n",
    "* *association rule learning*: discover interesting relations between attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semi-supervised learning\n",
    "\n",
    "* algos taking paritally labeled data, usu. lots of unlabeled data and a little labeled\n",
    "  - ex: step 1 unsupervised clustering of faces from an unlabeled set of photos; step 2 human assigns a name label per face\n",
    "  - most are combinations of supervised and unsupervised algos\n",
    "* deep belief networks (DBNs) are based on unsupervised components called restricted Bolzmann machines (RBMs) stack on top of one another"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement learning\n",
    "\n",
    "* very different\n",
    "* learning system (called an *agent*) can observe an environment, select and perform actions; gets *rewards* or *penalties* for its choices\n",
    "* creates a *policy* to get the most reward over time; a policy defines what actions an agent takes in a given situation\n",
    "* ex: robots learning to walk and AlphaGo (just applying the policy it had learned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch and online learning\n",
    "\n",
    "* whether or not it can learn incrementally\n",
    "\n",
    "## Batch learning\n",
    "\n",
    "* takes all data, trains algo, takes a lot of time\n",
    "* also called offline learning\n",
    "* then launched, no more learning\n",
    "\n",
    "## Online learning\n",
    "\n",
    "* system trained incrementally, either individually or *mini-batches*\n",
    "* good for data w/ continuous data flow and need to react rapidly (eg, stock prices); also for limited computing resources\n",
    "* can also be used for *out-of-core* learning\n",
    "* *learning rate*: how fast to adapt to changing data (and thus forget old data)\n",
    "  - high learning rate means adapt quickly but forget old data\n",
    "  - low means more inertia, less sensitive to noise in new data or non-representative data points\n",
    "* danger of bad data input degrading system; may want to monitor input and react to abnormal data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instance based v Model based learning\n",
    "\n",
    "Distinction on how ML systems *generalize* (how it learns to perform on instances never seen)\n",
    "\n",
    "## Instance based learning\n",
    "\n",
    "* a system starts by looking for exact matches, then generalizes to similar examples\n",
    "* *measure of similarity*: ex: count of words in common between two docs (spam and unknown)\n",
    "\n",
    "## Model based learning\n",
    "\n",
    "* generalize from a set of examples, then make predictions\n",
    "* How to know which model performs best\n",
    "  - *utility function* (or *fitness function*) that measures how good your model is\n",
    "  - *cost function* measures how bad it is\n",
    "  - for linear regression, usu. use a cost function to measure distance between model's preductions and training examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Main challenges of Machine Learning\n",
    "\n",
    "## Insufficient data\n",
    "\n",
    "On effectiveness of data:\n",
    "* Banko and Brill (2001): very diff algos perform similarly w/ enough data\n",
    "* Peter Norvig et al (2009). \"The Unreasonable Effectiveness of Data\". Data is more important than algo for complex problems; thus invest in corpus.\n",
    "\n",
    "## Non-representative training data\n",
    "\n",
    "* training data too small, you get *sample noise*\n",
    "* for large sets: if sampling method is flawed, we get *sampling bias*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Poor data quality\n",
    "\n",
    "* rm or fix outliers\n",
    "* if some instances are missing a few features, must decide whether to drop instance, fill in w/ average, ignore the attribute for all instances, or train 2 models, w/ and w/o the missing value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Irrelevant features\n",
    "\n",
    "* *feature engineering*: deciding upon good features\n",
    "  - *feature selection*: selecting more useful features\n",
    "  - *feature extraction*: combining features to produce a more useful one (dimensionality reduction helps)\n",
    "  - creating new features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overfitting the training data\n",
    "\n",
    "* *overfitting*: model performs well on training data but does not generalize\n",
    "  - happens when model is too complex relative to amount and/or noisiness of training data\n",
    "* solutions:\n",
    "  - simplify the model by selecting simpler algo, algo w/ fewer parameters, reducing number of attributes in training data, constraining model\n",
    "  - get more training data\n",
    "  - reduce noise in training data (fix errors, rm outliers)\n",
    "* *regularization*: constraining model to make is simpler, thus reducing overfitting\n",
    "* *degrees of freedom*: number of variables is an algo. Ex: `z = Ax + y` has two degrees of freedome; however if `x` is limited to some range, then it'll have between 1 and 2 degrees of freedom\n",
    "* *hyperparameter*: controlls the amount of regularization; a parameter of the algo, not the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Underfitting the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *underfitting*: when the model is too simple to learn from the training data\n",
    "* to fix underfitting:\n",
    "  - use more powerful model with more parameters\n",
    "  - feature engineering -- better features into model\n",
    "  - reduce constraints on the model (eg reducing regularization hyperparameter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing and validation\n",
    "\n",
    "* *training set*: what you make model from\n",
    "* *test set*: what you evaulate model from\n",
    "* *generalization error* (or *out-of-sample error*): estimation of accuracy of model on unseen data\n",
    "* *validation set*: a second holdout set in addition to test set\n",
    "* *cross-validation*: divide all data into complementary test/train sets, making multple passes with different divisions; average errors; train final model on all data\n",
    "* *no free lunch theorem*: Wolpert (1996): there is no model that is guaranteed to work a prior better than another, must try them all"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
